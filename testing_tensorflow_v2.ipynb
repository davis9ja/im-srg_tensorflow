{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davis9ja/im-srg_tensorflow/blob/master/testing_tensorflow_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "12WjV8yIyAb0",
    "outputId": "3ddb6ce6-28af-434f-bce6-dcaa221a307a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQZeGM1NyHBk"
   },
   "outputs": [],
   "source": [
    "# --- BUILD HAMILTONIAN -----------\n",
    "ref = np.asarray([1,1,1,1,0,0,0,0])\n",
    "holes = np.asarray([0,1,2,3])\n",
    "particles = np.asarray([4,5,6,7])\n",
    "\n",
    "# one-body basis is just all possible single particle states\n",
    "# B1 = tf.constant(holes+particles)#np.append(holes,particles)\n",
    "B1 = np.append(holes,particles)\n",
    "\n",
    "# we build two-body basis with all possible pairs of single particle states\n",
    "B2 = []\n",
    "ind2B = {}\n",
    "\n",
    "count = 0\n",
    "for h in holes:\n",
    "    for h2 in holes:\n",
    "        B2.append((h,h2))\n",
    "        ind2B[(h,h2)] = count\n",
    "        count += 1\n",
    "\n",
    "for h in holes:\n",
    "    for p in particles:\n",
    "        B2.append((h,p))\n",
    "        ind2B[(h,p)] = count\n",
    "        count += 1\n",
    "        \n",
    "for p in particles:\n",
    "    for h in holes:\n",
    "        B2.append((p,h))\n",
    "        ind2B[(p,h)] = count\n",
    "        count += 1\n",
    "            \n",
    "for p in particles:\n",
    "    for p2 in particles:\n",
    "        B2.append((p,p2)) \n",
    "        ind2B[(p,p2)] = count\n",
    "        count += 1           \n",
    "\n",
    "# the one-body Hamiltonian is diagonal, with elements given by P-1 or \n",
    "# single particle state index divided by 2 (rounding down)\n",
    "H1B = np.zeros((len(B1), len(B1)))\n",
    "for i in range(len(B1)):\n",
    "    H1B[i,i] = np.floor_divide(i,2)\n",
    "\n",
    "# the two-body Hamiltonian is sparse; only non-zero contributions \n",
    "# where pq=rs or pq=sr=-rs\n",
    "H2B = np.zeros((len(B2), len(B2)))\n",
    "for i in range(len(B2)):\n",
    "    for j in range(len(B2)):\n",
    "        p, q = B2[i]\n",
    "        r, s = B2[j]\n",
    "        \n",
    "        pp = np.floor_divide(p,2)\n",
    "        qp = np.floor_divide(q,2)\n",
    "        rp = np.floor_divide(r,2)\n",
    "        sp = np.floor_divide(s,2)\n",
    "        \n",
    "        ps = 1 if p%2==0 else -1\n",
    "        qs = 1 if q%2==0 else -1\n",
    "        rs = 1 if r%2==0 else -1\n",
    "        ss = 1 if s%2==0 else -1\n",
    "        \n",
    "        if pp != qp or rp != sp:\n",
    "            continue\n",
    "        if ps == qs or rs == ss:\n",
    "            continue\n",
    "        if ps == rs and qs == ss:\n",
    "            H2B[i,j] = -0.25\n",
    "        if ps == ss and qs == rs:\n",
    "            H2B[i,j] = 0.25\n",
    "\n",
    "B1_len = len(B1)\n",
    "H2B_t = np.zeros((B1_len,B1_len,B1_len,B1_len))\n",
    "for p in B1:\n",
    "    for q in B1:\n",
    "        for r in B1:\n",
    "            for s in B1:\n",
    "                H2B_t[p,q,r,s] = H2B[ind2B[(p,q)], ind2B[(r,s)]]\n",
    "        \n",
    "# test2 = np.einsum('ijij->ij',H2B_t)\n",
    "# print(test2)\n",
    "# for i in holes:\n",
    "#   for j in holes:\n",
    "#         print(H2B[ind2B[(i,j)], ind2B[(i,j)]])\n",
    "#         print(H2B_t[i,j,i,j])\n",
    "# print(np.shape(list(map(lambda i: H2B_t[i,:,i,:], holes))))\n",
    "# test = list(map(lambda i,j: H2B_t[i,j,i,j],holes,particles))\n",
    "# print(np.shape(test))\n",
    "\n",
    "# covers na - nb\n",
    "occA = np.zeros_like(H2B_t)\n",
    "for a in B1:\n",
    "    for b in B1:\n",
    "        occA[a,b,a,b] = ref[a] - ref[b]\n",
    "        \n",
    "# covers (1-na-nb)\n",
    "occB = np.zeros_like(H2B_t)\n",
    "for a in B1:\n",
    "    for b in B1:\n",
    "        occB[a,b,a,b] = 1 - ref[a] - ref[b]\n",
    "        \n",
    "# covers na*nb + (1-na-nb)*nc\n",
    "occC = np.zeros((B1_len,B1_len,B1_len,B1_len,B1_len,B1_len))\n",
    "for a in B1:\n",
    "    for b in B1:\n",
    "        for c in B1:\n",
    "            occC[a,b,c,a,b,c] = ref[a]*ref[b] + (1-ref[a]-ref[b])*ref[c]\n",
    "\n",
    "# covers na*nb*(1-nc-nd) + na*nb*nc*nd\n",
    "occD = np.zeros((B1_len,B1_len,B1_len,B1_len,B1_len,B1_len,B1_len,B1_len))\n",
    "for a in B1:\n",
    "    for b in B1:\n",
    "        for c in B1:\n",
    "            for d in B1:\n",
    "                occD[a,b,c,d,a,b,c,d] = ref[a]*ref[b]*(1-ref[c]-ref[d])+ref[a]*ref[b]*ref[c]*ref[d]\n",
    "\n",
    "ref_t = tf.convert_to_tensor(ref)\n",
    "\n",
    "B1_t = tf.convert_to_tensor(B1, dtype=tf.int32)\n",
    "holes = tf.convert_to_tensor(holes, dtype=tf.int32)\n",
    "particles = tf.convert_to_tensor(particles, dtype=tf.int32)\n",
    "\n",
    "H1B_t = tf.convert_to_tensor(H1B, dtype=tf.float32)\n",
    "H2B_t = tf.convert_to_tensor(H2B_t, dtype=tf.float32)\n",
    "\n",
    "occA_t = tf.convert_to_tensor(occA, dtype=tf.float32)\n",
    "occB_t = tf.convert_to_tensor(occB, dtype=tf.float32)\n",
    "occC_t = tf.convert_to_tensor(occC, dtype=tf.float32)\n",
    "occD_t = tf.convert_to_tensor(occD, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "t-gCmAPvyp4z",
    "outputId": "5c7228c6-8225-4e6b-9958-74543aa7156f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jacob/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# --- NORMAL ORDER HAMILTONIAN -----------\n",
    "# Calculate 0b, 1b, 2b pieces \n",
    "#\n",
    "# zero-body piece is scalar\n",
    "# one-body piece is rank 2 tensor\n",
    "# two-body piece is rank 4 tensor\n",
    "\n",
    "# - Calculate 0b tensor\n",
    "# E = tf.Variable(0.0)\n",
    "contr_1b = tf.map_fn(lambda i: H1B_t[i,i], holes, dtype=tf.float32)\n",
    "contr_2b = tf.map_fn(lambda i: H2B_t[i,:,i,:], holes, dtype=tf.float32)\n",
    "\n",
    "E_1b = tf.reduce_sum(contr_1b, 0)\n",
    "E_2b = 0.5*tf.reduce_sum(contr_2b, [0,1,2])\n",
    "E = tf.add_n([E_1b, E_2b])\n",
    "\n",
    "# - Calculate 1b tensor\n",
    "# f = tf.Variable(tf.identity(H1B_t))\n",
    "# f = tf.identity(H1B_t)\n",
    "contr_2b = tf.map_fn(lambda i: H2B_t[:,i,:,i], holes, dtype=tf.float32)\n",
    "contr_2b = tf.reduce_sum(contr_2b,0) # sum over holes\n",
    "\n",
    "f = tf.add_n([H1B_t, contr_2b])\n",
    "\n",
    "# - Calculate 2b tensor\n",
    "G = tf.identity(H2B_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwn3kw6yeUQO"
   },
   "outputs": [],
   "source": [
    "# --- SET UP WHITE'S GENERATOR -----------\n",
    "\n",
    "# TODO: try reduce_sum, einsum, tensordot?, other methods\n",
    "\n",
    "def white(f, G, holes, particles):\n",
    "  \n",
    "    # - Calculate 1b generator tensor\n",
    "    eta1B = tf.identity(f)\n",
    "    eta1Bph = eta1B[particles[0]:particles[-1]+1,holes[0]:holes[-1]+1]\n",
    "    fhh = f[holes[0]:holes[-1]+1,holes[0]:holes[-1]+1]\n",
    "    fpp = f[particles[0]:particles[-1]+1,particles[0]:particles[-1]+1]\n",
    "    Gphph = G[particles[0]:particles[-1]+1, holes[0]:holes[-1]+1, particles[0]:particles[-1]+1, holes[0]:holes[-1]+1]\n",
    "    Gphph = tf.reduce_sum(Gphph,[0,1]) # reduce to same rank as f\n",
    "    \n",
    "    Deltaph = tf.subtract(fpp, tf.add(fhh,Gphph))\n",
    "    temp = tf.div_no_nan(eta1Bph,Deltaph)\n",
    "    \n",
    "    top_block = tf.zeros((4,4))\n",
    "    bottom_block = tf.zeros((4,8))\n",
    "    conc_top = tf.concat([top_block, temp],1)\n",
    "    eta1B = tf.concat([conc_top, bottom_block], 0)\n",
    "    \n",
    "    # - Calculate 2b generator tensor\n",
    "    eta2B = tf.zeros(G.shape)\n",
    "#     return temp\n",
    "    return eta1B\n",
    "#     return (eta1B, eta2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SET UP WEGNER'S GENERATOR -----------\n",
    "\n",
    "def wegner(f, G):\n",
    "# basis = tf.concat([holes, particles],0)\n",
    "\n",
    "    # --- Need to decouple diagonal and off-diagonal elements; procedure in Ch.10 AACCNP\n",
    "\n",
    "    # Decoupling 1B piece\n",
    "    # indices are constructed by all possible combinations of particle-hole(hole-particle) states\n",
    "    col_indices =tf.reshape(tf.concat([tf.broadcast_to(particles,[4,4]),tf.broadcast_to(holes,[4,4])],0),[-1])\n",
    "    row_indices = tf.reshape(tf.transpose(tf.concat([tf.broadcast_to(holes,[4,4]),tf.broadcast_to(particles,[4,4])],1)),[-1])\n",
    "    ph_indices = tf.stack([row_indices, col_indices], axis=1)\n",
    "    ph_updates = tf.gather_nd(f, ph_indices)\n",
    "\n",
    "    fod = tf.scatter_nd(ph_indices,ph_updates,f.shape)\n",
    "    fd = tf.subtract(f,fod)\n",
    "\n",
    "    # Decoupling 2B piece\n",
    "    # indices are constructed by all possible combinations of pphh(hhpp) states\n",
    "    ind1_C = tf.concat([tf.broadcast_to(holes,[64,4]), tf.broadcast_to(particles,[64,4])],1)\n",
    "    ind1_TC = tf.transpose(ind1_C) \n",
    "    ind1 = tf.reshape(ind1_TC,[-1])\n",
    "\n",
    "    ind2_C = tf.concat([tf.broadcast_to(holes,[16,16]),tf.broadcast_to(particles,[16,16])],1)\n",
    "    ind2_TC = tf.transpose(ind2_C)\n",
    "    ind2 = tf.reshape(ind2_TC,[-1])\n",
    "\n",
    "    ind3_C = tf.concat([tf.broadcast_to(particles,[4,64]),tf.broadcast_to(holes,[4,64])],1)\n",
    "    ind3_TC = tf.transpose(ind3_C) \n",
    "    ind3 = tf.reshape(ind3_TC,[-1])\n",
    "\n",
    "    ind4_C = tf.concat([tf.broadcast_to(particles,[1,256]),tf.broadcast_to(holes,[1,256])],1)\n",
    "    ind4_TC = tf.transpose(ind4_C)\n",
    "    ind4 = tf.reshape(ind4_TC,[-1])\n",
    "\n",
    "    pphh_indices = tf.stack([ind1,ind2,ind3,ind4],axis=1)\n",
    "    pphh_updates = tf.gather_nd(G, pphh_indices)\n",
    "\n",
    "    God = tf.scatter_nd(pphh_indices,pphh_updates,G.shape)\n",
    "    Gd = tf.subtract(G,God)\n",
    "\n",
    "\n",
    "    # --- 1B piece\n",
    "#     eta1B = tf.Variable(tf.zeros(f.shape))\n",
    "\n",
    "    # Calculate 1B-1B contribution\n",
    "    fd_fod = tf.tensordot(fd,fod,1)\n",
    "    fd_fod_T = tf.transpose(fd_fod)\n",
    "    eta1B_1b1b = tf.subtract(fd_fod, fd_fod_T)\n",
    "\n",
    "    # fod_fd = tf.tensordot(fod,fd,1)\n",
    "    # eta1B = tf.assign_add(eta1B, tf.subtract(fd_fod, fod_fd))\n",
    "\n",
    "\n",
    "    # Calculate 1B-2B contribution\n",
    "    fd_God = tf.tensordot(fd, tf.tensordot(occA_t,God,([0,1],[2,0])),([0,1],[2,0]))\n",
    "    fod_Gd = tf.tensordot(fod, tf.tensordot(occA_t,Gd,([0,1],[2,0])),([0,1],[2,0]))\n",
    "    eta1B_1b2b = tf.subtract(fd_God, fod_Gd)\n",
    "\n",
    "\n",
    "    # Calculate 2B-2B contribution\n",
    "    Gd_God = tf.tensordot(Gd, tf.tensordot(occC_t,God,([0,1,2],[0,1,2])),([2,3,1],[0,1,2]))\n",
    "    Gd_God_T = tf.transpose(Gd_God)\n",
    "    scaled_sub = tf.scalar_mul(tf.constant(0.5),tf.subtract(Gd_God,Gd_God_T))\n",
    "    eta1B_2b2b = scaled_sub\n",
    "    \n",
    "    eta1B = tf.add_n([eta1B_1b1b, eta1B_1b2b, eta1B_2b2b])\n",
    "\n",
    "    #     God_Gd = tf.tensordot(God, tf.tensordot(occC_t,Gd,([0,1,2],[0,1,2])),([0,1,2],[2,3,1]))\n",
    "    #     scaled_sub = tf.scalar_mul(tf.constant(0.5),tf.subtract(Gd_God,God_Gd))\n",
    "    #     eta1B = tf.assign_add(eta1B, scaled_sub)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- 2B piece\n",
    "#     eta2B = tf.Variable(tf.zeros(G.shape))\n",
    "\n",
    "    # Calculate 1B-2B contribution\n",
    "    fdGod_fodGd_ij = tf.subtract( tf.tensordot(fd,God,[[1],[0]]), tf.tensordot(fod,Gd,[[1],[0]]) )\n",
    "    fdGod_fodGd_ij_T = tf.transpose(fdGod_fodGd_ij, perm=[1,0,2,3])\n",
    "    ij_term = tf.subtract(fdGod_fodGd_ij,fdGod_fodGd_ij_T)\n",
    "\n",
    "    # Godfd_Gdfod_ij = tf.subtract( tf.tensordot(God,fd,([0],[1])), tf.tensordot(Gd,fod,([0],[1])) )\n",
    "    # ij_term = tf.subtract(fdGod_fodGd_ij,Godfd_Gdfod_ij)\n",
    "\n",
    "    fdGod_fodGd_kl = tf.subtract( tf.tensordot(fd,God,[[0],[2]]), tf.tensordot(fod,Gd,[[0],[2]]) )\n",
    "    fdGod_fodGd_kl = tf.transpose(fdGod_fodGd_kl,perm=[1,2,0,3]) # permute back to i,j,k,l order\n",
    "    fdGod_fodGd_kl_T = tf.transpose(fdGod_fodGd_kl,perm=[0,1,3,2])\n",
    "    kl_term = tf.subtract(fdGod_fodGd_kl,fdGod_fodGd_kl_T)\n",
    "\n",
    "    # Godfd_Gdfod_kl = tf.subtract( tf.tensordot(God,fd,([2],[0])), tf.tensordot(Gd,fod,([2],[0])) )\n",
    "    # kl_term = tf.subtract(fdGod_fodGd_kl,Godfd_Gdfod_kl)\n",
    "\n",
    "    eta2B_1b2b = tf.subtract(ij_term,kl_term)\n",
    "\n",
    "\n",
    "    # Calculate 2B-2B contribution\n",
    "    # GdGod_GodGd = tf.subtract( tf.tensordot(Gd,God,[[2,3],[0,1]]), tf.tensordot(God,Gd,[[2,3],[0,1]]) )\n",
    "    # GdGod_GodGd_occB = tf.tensordot(occB_t,GdGod_GodGd,[[2,3],[0,1]])\n",
    "\n",
    "    GdGod_occB = tf.tensordot(Gd, tf.tensordot(occB_t, God, [[0,1],[0,1]]), [[2,3],[0,1]])\n",
    "    GodGd_occB = tf.tensordot(God, tf.tensordot(occB_t, Gd, [[0,1],[0,1]]), [[2,3],[0,1]])\n",
    "    scaled_sub = tf.scalar_mul(tf.constant(0.5),tf.subtract(GdGod_occB,GodGd_occB))\n",
    "    # GdGod_occB = tf.tensordot( Gd, tf.tensordot(occB_t,God,[[0,1],[0,1]]), [[2,3],[0,1]] )\n",
    "    # GodGd_occB = tf.tensordot( God, tf.tensordot(occB_t,Gd,[[0,1],[0,1]]), [[2,3],[0,1]] )\n",
    "    # scaled_sub = tf.scalar_mul(tf.constant(0.5), tf.subtract(GdGod_occB,GodGd_occB))\n",
    "    eta2B_2b2b_B = scaled_sub\n",
    "\n",
    "    GdGod = tf.tensordot(Gd,God,[[0,2],[2,0]])\n",
    "    GdGod = tf.transpose(GdGod,perm=[0,2,1,3]) # permute back to i,j,k,l order\n",
    "    GdGod_occA = tf.tensordot(occA_t,GdGod,[[2,3],[0,1]])\n",
    "    # GdGod_occA = tf.tensordot( Gd, tf.tensordot(occA_t,God,[[0,1],[2,0]]), [[0,2],[0,1]] )\n",
    "    # GdGod_occA = tf.transpose(GdGod_occA, perm=[0,2,1,3]) # permute back to i,j,k,l order\n",
    "    GdGod_occA_Tij = tf.transpose(GdGod_occA,perm=[1,0,2,3])\n",
    "    GdGod_occA_Tkl = tf.transpose(GdGod_occA,perm=[0,1,3,2])\n",
    "    GdGod_occA_Tijkl = tf.transpose(GdGod_occA,perm=[1,0,3,2])\n",
    "    sub1 = tf.subtract(GdGod_occA,GdGod_occA_Tij)\n",
    "    sub2 = tf.subtract(sub1,GdGod_occA_Tkl)\n",
    "    add3 = tf.add(sub2,GdGod_occA_Tijkl)\n",
    "\n",
    "    eta2B_2b2b_A = add3\n",
    "    \n",
    "    eta2B = tf.add_n([eta2B_1b2b, eta2B_2b2b_B, eta2B_2b2b_A])\n",
    "    \n",
    "    return (eta1B, eta2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE FLOW EQUATIONS -----------\n",
    "\n",
    "def flow(f, G, eta1B, eta2B):\n",
    "#     eta1B, eta2B = wegner(f,G)\n",
    "\n",
    "    # --- 0B piece\n",
    "    # dE = tf.Variable(0.0)\n",
    "\n",
    "    # Calculate 1B-1B contribution (full contraction)\n",
    "    occA_e1 = tf.tensordot(occA_t, eta1B, [[0,1],[0,1]])\n",
    "    occA_e1_f = tf.tensordot(occA_e1, f, [[0,1],[1,0]])\n",
    "    dE_1b1b = tf.identity(occA_e1_f)\n",
    "\n",
    "    # Calculate 2B-2B contribution (full contraction)\n",
    "    e2_occD = tf.tensordot(eta2B, occD_t, [[0,1,2,3],[0,1,2,3]])\n",
    "    e2_occD_G = 0.5*tf.tensordot(e2_occD, G, [[0,1,2,3],[2,3,0,1]])\n",
    "\n",
    "    # e2_occD_G = tf.scalar_mul(tf.constant(0.5),e2_occD_G)\n",
    "    dE_2b2b = tf.identity(e2_occD_G)\n",
    "\n",
    "    dE = tf.add_n([dE_1b1b, dE_2b2b])\n",
    "\n",
    "    # --- 1B piece\n",
    "    # df = tf.Variable(tf.zeros(f.shape))\n",
    "\n",
    "    # Calculate 1B-1B contribution (contraction over 1 index)\n",
    "    e1_f = tf.tensordot(eta1B,f,[[1],[0]])\n",
    "    e1_f_T = tf.transpose(e1_f)\n",
    "    e1_f_add = tf.add(e1_f,e1_f_T)\n",
    "    df_1b1b = tf.identity(e1_f_add)\n",
    "\n",
    "    # Calculate 1B-2B contribution (contraction over 2 indices)\n",
    "    occA_e1_G = tf.tensordot(occA_t, tf.tensordot(eta1B,G,[[0,1],[2,0]]), [[2,3],[0,1]])\n",
    "    occA_f_e2 = tf.tensordot(occA_t, tf.tensordot(f,eta2B,[[0,1],[2,0]]), [[2,3],[0,1]])\n",
    "    sub_1b2b = tf.subtract(occA_e1_G, occA_f_e2)\n",
    "    df_1b2b = tf.identity(sub_1b2b)\n",
    "\n",
    "    # Calculate 2B-2B contribution (contraction over 3 indices)\n",
    "    e2_occC_G = tf.tensordot(eta2B, tf.tensordot(occC_t,G,[[3,4,5],[0,1,2]]), [[2,3,0],[0,1,2]])\n",
    "    e2_occC_G_T = tf.transpose(e2_occC_G)\n",
    "    add_2b2b = 0.5*tf.add(e2_occC_G,e2_occC_G_T)\n",
    "    df_2b2b = tf.identity(add_2b2b)\n",
    "\n",
    "    df = tf.add_n([df_1b1b, df_1b2b, df_2b2b])\n",
    "\n",
    "    # --- 2B piece\n",
    "    # dG = tf.Variable(tf.zeros(G.shape))\n",
    "\n",
    "    # Calculate 1B-2B contribution (contraction over 1 index)\n",
    "    e1G_fe2_ij = tf.subtract(tf.tensordot(eta1B,G,[[1],[0]]), tf.tensordot(f,eta2B,[[1],[0]]))\n",
    "    e1G_fe2_ij_T = tf.transpose(e1G_fe2_ij, perm=[1,0,2,3])\n",
    "    ij_term = tf.subtract(e1G_fe2_ij,e1G_fe2_ij_T)\n",
    "\n",
    "    e1G_fe2_kl = tf.subtract(tf.tensordot(eta1B,G,[[0],[2]]), tf.tensordot(f,eta2B,[[0],[2]]))\n",
    "    e1G_fe2_kl = tf.transpose(e1G_fe2_kl, perm=[1,2,0,3]) # permute to i,j,k,l order\n",
    "    e1G_fe2_kl_T = tf.transpose(e1G_fe2_kl, perm=[0,1,3,2])\n",
    "    kl_term = tf.subtract(e1G_fe2_kl,e1G_fe2_kl_T)\n",
    "\n",
    "    dG_1b2b = tf.identity(tf.subtract(ij_term, kl_term))\n",
    "\n",
    "    # Calculate 2B-2B contribution (occB term)\n",
    "    e2G_Ge2 = tf.subtract(tf.tensordot(eta2B,G,[[2,3],[0,1]]), tf.tensordot(G,eta2B,[[2,3],[0,1]]))\n",
    "    e2G_Ge2_occB = 0.5*tf.tensordot(occB_t,e2G_Ge2,[[2,3],[0,1]])\n",
    "\n",
    "    dG_2b2b_B = tf.identity(e2G_Ge2_occB)\n",
    "\n",
    "    # Calculate 2B-2B contribution (occA term)\n",
    "    e2G = tf.tensordot(eta2B, G, [[0,2],[2,0]])\n",
    "    e2G = tf.transpose(e2G, perm=[0,2,1,3]) # permute back to i,j,k,l order\n",
    "    e2G_occA = tf.tensordot(occA_t, e2G, [[2,3],[0,1]])\n",
    "    e2G_occA_Tij = tf.transpose(e2G_occA, perm=[1,0,2,3])\n",
    "    e2G_occA_Tkl = tf.transpose(e2G_occA, perm=[0,1,3,2])\n",
    "    e2G_occA_Tijkl = tf.transpose(e2G_occA, perm=[1,0,3,2])\n",
    "    sub1 = tf.subtract(e2G_occA, e2G_occA_Tij)\n",
    "    sub2 = tf.subtract(sub1, e2G_occA_Tkl)\n",
    "    add3 = tf.add(sub2, e2G_occA_Tijkl)\n",
    "\n",
    "    dG_2b2b_A = tf.identity(add3)\n",
    "\n",
    "    dG = tf.add_n([dG_1b2b, dG_2b2b_B, dG_2b2b_A])\n",
    "\n",
    "    return (dE, df, dG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE DERIVATIVE TO PASS INTO ODEINT SOLVER -----------\n",
    "# The input y is an N-D tensor; trying to pass y as a TensorFlow.tuple\n",
    "# containing E, f, and G\n",
    "\n",
    "def derivative(t, y):\n",
    "    \n",
    "    E, f, G = y\n",
    "    \n",
    "    eta1B, eta2B = wegner(f, G)\n",
    "    \n",
    "    dE, df, dG = flow(f, G)\n",
    "    \n",
    "    dy = tf.tuple([dE, df, dG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(E, f, G):\n",
    "    unravel_E = tf.reshape(E, [-1])\n",
    "    unravel_f = tf.reshape(f, [-1])\n",
    "    unravel_G = tf.reshape(G, [-1])\n",
    "    \n",
    "    return tf.concat([unravel_E, unravel_f, unravel_G], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qqW27CGI0FF8",
    "outputId": "dbfb500f-ea0a-4e0d-f8d2-659b5719aa1b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "# --- CAUTION: DO NOT EVALUATE PAST/FUTURE GRAPHS IN A SINGLE SESSION WHEN THOSE GRAPHS ARE DEPENDENT ---\n",
    "# e.g., evaluating dE also evaluates eta2B,eta1B; if you try to evaluate eta2B,eta1B again, you will \n",
    "#       receive an unexpected result\n",
    "# \n",
    "# --- The workaround is to use the var_print() method to \"print\" the value of the tensor you want;\n",
    "#     I think the issue has something to do with tf.Variable and the assign_add() method. In the \n",
    "#     future, I would like to fix the issue. For now, though, I just want a running code.\n",
    "#\n",
    "# ************ ABOVE ISSUE HAS BEEN FIXED; WILL CLOSE OUT WARNING IN RELEASE VERSION *****************\n",
    "\n",
    "# def var_print(session, tensor):\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     print(session.run(tensor))\n",
    "\n",
    "test = tf.constant([1,2])\n",
    "test2 = tf.constant([3])\n",
    "test3 = tf.constant(3)\n",
    "tuple_t = tf.tuple([test,test2,test3]) \n",
    "\n",
    "one,two,three = tuple_t\n",
    "\n",
    "s = tf.linspace(0.,7.,100)\n",
    "y0 = tf.tuple([E, f, G])\n",
    "# solns = tf.contrib.integrate.odeint(derivative, y0, tf.constant([1,2]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     print(dE.eval())\n",
    "#     print(df.eval())\n",
    "    print(unravel(test3,test2,test).eval())\n",
    "    \n",
    "#     print(dG[0,1,5,4].eval())\n",
    "\n",
    "#     test = np.zeros((8,8,8,8))\n",
    "#     test1 = np.zeros((8,8,8,8))\n",
    "#     test2 = np.zeros_like(test)\n",
    "#     test3 = np.zeros_like(test)\n",
    "#     test4 = np.zeros_like(test)\n",
    "#     eta2B_l = eta2B.eval()\n",
    "#     for i in B1:\n",
    "#         for j in B1:\n",
    "#             for k in B1:\n",
    "#                 for l in B1:\n",
    "# #                     for a in B1:\n",
    "#                     if(eta2B_l[i,j,k,l]>0):\n",
    "#                         print(eta2B_l[i,j,k,l],i,j,k,l)\n",
    "# #                         if(fd_l[a,k]*God_l[i,j,a,l] < 0 or fd_l[a,k]*God_l[i,j,a,l] > 0):\n",
    "# #                             print(fd_l[a,k]*God_l[i,j,a,l], a,\"|\",i,j,k,l)\n",
    "#                         test1[i,j,k,l] += fd_l[i,a]*God_l[a,j,k,l]\n",
    "#                         test2[i,j,k,l] += fd_l[j,a]*God_l[a,i,k,l]\n",
    "#                         test3[i,j,k,l] += fd_l[a,k]*God_l[i,j,a,l]\n",
    "#                         test4[i,j,k,l] += fd_l[a,l]*God_l[i,j,a,k]\n",
    "                        \n",
    "#                         test[i,j,k,l] += fd_l[i,a]*God_l[a,j,k,l] \\\n",
    "#                                        - fd_l[j,a]*God_l[a,i,k,l] \\\n",
    "#                                        - fd_l[a,k]*God_l[i,j,a,l] \\\n",
    "#                                        + fd_l[a,l]*God_l[i,j,a,k]\n",
    "#     print(test[0,1,4,5])\n",
    "#     print(test1[0,1,4,5],test2[0,1,4,5],test3[0,1,4,5],test4[0,1,4,5])\n",
    "# #     print(tf.transpose(test,perm=[0,1,3,2])[0,1,4,5].eval())\n",
    "    \n",
    "#     test_con1 = tf.tensordot(fd,God,[[1],[0]])\n",
    "#     test_con2 = tf.transpose(test_con1,perm=[1,0,2,3])\n",
    "#     test_con3 = tf.transpose(tf.tensordot(fd,God,[[0],[2]]),perm=[1,2,0,3])\n",
    "#     test_con4 = tf.transpose(test_con3,perm=[0,1,3,2])\n",
    "#     test_con = tf.add(tf.subtract(tf.subtract(test_con1,test_con2),test_con3),test_con4)\n",
    "#     print(test_con1[0,1,4,5].eval(), test_con2[0,1,4,5].eval(),test_con3[0,1,4,5].eval(),test_con4[0,1,4,5].eval())\n",
    "#     print(test_con[0,1,4,5].eval())\n",
    "#     print(\"true,\",test3[0,1,4,5])\n",
    "#     print(\"test,\",test_con3[0,1,4,5].eval())\n",
    "#     print(\"tf,\",tf.tensordot(fd,God,[[0],[2]])[4,0,1,5].eval())\n",
    "#     print(\"tf_p,\", tf.transpose(tf.tensordot(fd,God,[[0],[2]]),perm=[1,2,0,3])[0,1,4,5].eval())\n",
    "#     print(\"numpy,\",np.tensordot(fd_l,God_l,axes=((0),(2)))[0,1,4,5])\n",
    "#     print(tf.transpose(fdGod_fodGd_kl,perm=[1,2,0,3])[0,1,4,5].eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqTTHpAJ4JPT"
   },
   "outputs": [],
   "source": [
    "# A = tf.constant([[1,2,3,4,5,6,7,8],\n",
    "#                  [3,4,5,6,7,8,9,1], \n",
    "#                  [7,8,9,1,2,3,4,5], \n",
    "#                  [6,7,8,9,1,2,3,4],\n",
    "#                  [1,2,3,4,5,6,7,8],\n",
    "#                  [3,4,5,6,7,8,9,1], \n",
    "#                  [7,8,9,1,2,3,4,5], \n",
    "#                  [6,7,8,9,1,2,3,4] ])\n",
    "\n",
    "# # indices = tf.constant([4,5,6,7, 4,5,6,7, 4,5,6,7, 4,5,6,7, 0,1,2,3, 0,1,2,3, 0,1,2,3, 0,1,2,3])\n",
    "# indices =tf.reshape(tf.concat([tf.broadcast_to(particles,[4,4]),tf.broadcast_to(holes,[4,4])],0),[-1])\n",
    "\n",
    "# # prepare row indices\n",
    "# # row_indices = tf.range(tf.shape(indices)[0])\n",
    "# # row_indices = tf.constant([0,0,0,0, 1,1,1,1, 2,2,2,2, 3,3,3,3, 4,4,4,4, 5,5,5,5, 6,6,6,6, 7,7,7,7])\n",
    "# row_indices = tf.reshape(tf.transpose(tf.concat([tf.broadcast_to(holes,[4,4]),tf.broadcast_to(particles,[4,4])],1)),[-1])\n",
    "# # row_indices = tf.reshape(tf)\n",
    "# # zip row indices with column indices\n",
    "# full_indices = tf.stack([row_indices, indices], axis=1)\n",
    "\n",
    "# # retrieve values by indices\n",
    "# # indices = tf.transpose(full_indices)\n",
    "# # updates = tf.reshape(tf.gather_nd(A, full_indices),[2,2])\n",
    "# updates = tf.gather_nd(A, full_indices)\n",
    "# S = tf.scatter_nd(full_indices,updates,[8,8])\n",
    "\n",
    "# test = tf.scatter_nd([[[1,0,0,1],[1,0,0,0]]],[[1,2]],[8,8,8,8])\n",
    "# idx2b = tf.expand_dims(tf.stack([row_indices,row_indices,indices,indices],axis=1),0)\n",
    "# test2b = tf.zeros([8,8,8,8])\n",
    "# with tf.Session() as sess:\n",
    "# #     print(full_indices.eval())\n",
    "#     print(test[1,0,0,1].eval())\n",
    "#     print(test[1,0,0,0].eval())\n",
    "# #     print(tf.gather_nd([[[1,0,0,1],[1,0,0,0]]],test))\n",
    "#     print(A.eval())\n",
    "#     print(\"\\n\")\n",
    "#     print(S.eval())\n",
    "#     # print(full_indices.eval())\n",
    "#     # print(tf.transpose(full_indices))\n",
    "#     print(idx2b.eval())\n",
    "#     print(tf.gather_nd(test2b,idx2b).eval())\n",
    "# #     print(tf.concat([tf.broadcast_to(particles,[4,4]),tf.broadcast_to(holes,[4,4])],0).eval())\n",
    "# #     print(tf.reshape(tf.concat([tf.broadcast_to(particles,[4,4]),tf.broadcast_to(holes,[4,4])],0),[-1]).eval())\n",
    "# #     print(tf.reshape(tf.transpose(tf.broadcast_to(B1_t,[4,8])),[-1]).eval())\n",
    "# #     print(tf.broadcast_to(tf.concat([holes,particles],1),[4,4]).eval())\n",
    "# #     print(tf.reshape(test,[-1]).eval())\n",
    "# # session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant(1)\n",
    "# shape = a.shape\n",
    "# print(len(shape))\n",
    "\n",
    "# def off_diagonal(tensor):\n",
    "#     # returns off diagonal elements defined by indices ph,hp (1B) or pphh,hhpp (2B)\n",
    "#     shape = tensor.shape\n",
    "#     if(shape==() or len(shape)==1):\n",
    "#         return 0\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hols = np.asarray([0,1,2,3])\n",
    "# pars = np.asarray([4,5,6,7])\n",
    "\n",
    "# count = 0\n",
    "# for p in pars:\n",
    "#     for pp in pars:\n",
    "#         for h in hols:\n",
    "#             for hh in hols:\n",
    "# #                 print(p,pp,h,hh)\n",
    "#                 count += 1\n",
    "                \n",
    "# print(count)\n",
    "\n",
    "# indices are p,pp,h,hh\n",
    "# for every possible combination:\n",
    "#\n",
    "# p   --> switches every 64 states - 2^6\n",
    "# pp  --> switches every 16 states - 2^4\n",
    "# h   --> switches every 04 states - 2^2\n",
    "# hh  --> swithces every 01 states - 2^0\n",
    "\n",
    "# ind1_C = tf.concat([tf.broadcast_to(holes,[64,4]), tf.broadcast_to(particles,[64,4])],1)\n",
    "# ind1_TC = tf.transpose(ind1_C) \n",
    "# ind1 = tf.reshape(ind1_TC,[-1])\n",
    "\n",
    "# ind2_C = tf.concat([tf.broadcast_to(holes,[16,16]),tf.broadcast_to(particles,[16,16])],1)\n",
    "# ind2_TC = tf.transpose(ind2_C)\n",
    "# ind2 = tf.reshape(ind2_TC,[-1])\n",
    "\n",
    "# ind3_C = tf.concat([tf.broadcast_to(particles,[4,64]),tf.broadcast_to(holes,[4,64])],1)\n",
    "# ind3_TC = tf.transpose(ind3_C) \n",
    "# ind3 = tf.reshape(ind3_TC,[-1])\n",
    "\n",
    "# ind4_C = tf.concat([tf.broadcast_to(particles,[1,256]),tf.broadcast_to(holes,[1,256])],1)\n",
    "# ind4_TC = tf.transpose(ind4_C)\n",
    "# ind4 = tf.reshape(ind4_TC,[-1])\n",
    "\n",
    "# pphh_ind = tf.stack([ind1,ind2,ind3,ind4],axis=1)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(pphh_ind.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "testing_tensorflow_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
